#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‚å•†HDæ–‡ä»¶æ·±åº¦åˆ†æå·¥å…·
ç”¨äºç ´è§£å¤©é—®Blockçš„æ–‡ä»¶æ ¼å¼
"""

import os
import sys
import binascii
from pathlib import Path

def analyze_hd_file_basic(file_path):
    """åŸºç¡€åˆ†æHDæ–‡ä»¶"""
    print(f"ğŸ” åˆ†ææ–‡ä»¶: {Path(file_path).name}")

    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False

    file_size = os.path.getsize(file_path)
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚ ({file_size/1024:.2f} KB)")

    try:
        with open(file_path, 'rb') as f:
            # è¯»å–æ–‡ä»¶å†…å®¹
            data = f.read()

            # åˆ†æå‰64å­—èŠ‚ï¼ˆæ–‡ä»¶å¤´ï¼‰
            header = data[:64]
            print(f"\nğŸ“‹ æ–‡ä»¶å¤´åˆ†æ (å‰64å­—èŠ‚):")
            print("-" * 40)

            # åå…­è¿›åˆ¶æ˜¾ç¤º
            for i in range(0, min(64, len(header)), 16):
                line = header[i:i+16]
                hex_part = ' '.join(f'{b:02X}' for b in line)
                ascii_part = ''.join(chr(b) if 32 <= b <= 126 else '.' for b in line)
                print(f"{i:04X}: {hex_part:<48} |{ascii_part}|")

            # æŸ¥æ‰¾å­—ç¬¦ä¸²
            print(f"\nğŸ“ å¯è¯»å­—ç¬¦ä¸²:")
            print("-" * 40)
            strings = find_readable_strings(data)
            for string in strings[:10]:  # æ˜¾ç¤ºå‰10ä¸ªå­—ç¬¦ä¸²
                print(f"  '{string}'")

            # å­—èŠ‚ç»Ÿè®¡
            print(f"\nğŸ“Š å­—èŠ‚åˆ†å¸ƒç»Ÿè®¡:")
            print("-" * 40)
            byte_counts = [0] * 256
            for byte in data:
                byte_counts[byte] += 1

            # æ˜¾ç¤ºæœ€å¸¸è§çš„å­—èŠ‚
            sorted_bytes = sorted(enumerate(byte_counts), key=lambda x: x[1], reverse=True)
            for byte_val, count in sorted_bytes[:10]:
                if count > 0:
                    percentage = (count / file_size) * 100
                    print(f"  0x{byte_val:02X}: {count:5d} æ¬¡ ({percentage:.1f}%)")

            # è®¡ç®—ç†µå€¼ï¼ˆè¡¡é‡æ•°æ®çš„éšæœºæ€§ï¼‰
            entropy = calculate_entropy(data)
            print(f"\nğŸ² æ–‡ä»¶ç†µå€¼: {entropy:.3f}")
            if entropy > 7.5:
                print("  âš ï¸ é«˜ç†µå€¼ï¼Œå¯èƒ½æ˜¯åŠ å¯†æˆ–å‹ç¼©æ•°æ®")
            elif entropy < 5.0:
                print("  âœ… ä½ç†µå€¼ï¼Œå¯èƒ½æ˜¯æœªåŠ å¯†çš„ç»“æ„åŒ–æ•°æ®")
            else:
                print("  â„¹ï¸ ä¸­ç­‰ç†µå€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")

            return True

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return False

def find_readable_strings(data, min_length=4):
    """æŸ¥æ‰¾æ•°æ®ä¸­çš„å¯è¯»å­—ç¬¦ä¸²"""
    strings = []
    current_string = ""

    for byte in data:
        if 32 <= byte <= 126:  # å¯æ‰“å°ASCIIå­—ç¬¦
            current_string += chr(byte)
        else:
            if len(current_string) >= min_length:
                strings.append(current_string)
            current_string = ""

    # å¤„ç†æœ€åä¸€ä¸ªå­—ç¬¦ä¸²
    if len(current_string) >= min_length:
        strings.append(current_string)

    # å»é‡å¹¶æ’åº
    unique_strings = list(set(strings))
    unique_strings.sort(key=len, reverse=True)

    return unique_strings

def calculate_entropy(data):
    """è®¡ç®—æ•°æ®çš„é¦™å†œç†µ"""
    if not data:
        return 0

    # ç»Ÿè®¡æ¯ä¸ªå­—èŠ‚çš„å‡ºç°æ¬¡æ•°
    byte_counts = {}
    for byte in data:
        byte_counts[byte] = byte_counts.get(byte, 0) + 1

    # è®¡ç®—ç†µå€¼
    entropy = 0.0
    data_len = len(data)

    for count in byte_counts.values():
        probability = count / data_len
        if probability > 0:
            entropy -= probability * (probability.bit_length() - 1)

    return entropy

def compare_multiple_files():
    """æ¯”è¾ƒå¤šä¸ªHDæ–‡ä»¶"""
    print(f"\nğŸ”„ æ‰¹é‡æ–‡ä»¶æ¯”è¾ƒåˆ†æ:")
    print("=" * 60)

    # æŸ¥æ‰¾æ‰€æœ‰HDæ–‡ä»¶
    base_path = Path(r"d:\repositories\robot_dog\3.æ¡Œå® ç‹—ä»£ç åŠæŒ‡ä»¤é›†ï¼ˆ3ä»£~9ä»£çš„æŒ‡ä»¤é›†åŠè¯­éŸ³æ¨¡å—ä»£ç ç­‰ï¼‰\2.è…¾å“¥æ¯ä»£è¯­éŸ³æ¨¡å—å¼€æºä»£ç ")

    hd_files = []
    if base_path.exists():
        # éAIç‰ˆæœ¬
        non_ai_path = base_path / "éAI"
        if non_ai_path.exists():
            for file in non_ai_path.glob("*.hd"):
                hd_files.append(("éAI", file))

        # AIç‰ˆæœ¬
        ai_path = base_path / "AI"
        if ai_path.exists():
            for file in ai_path.glob("*.hd"):
                hd_files.append(("AI", file))

    if not hd_files:
        print("âŒ æœªæ‰¾åˆ°HDæ–‡ä»¶")
        return

    print(f"ğŸ“ æ‰¾åˆ° {len(hd_files)} ä¸ªHDæ–‡ä»¶:")
    file_info = {}

    for category, file_path in hd_files:
        print(f"\nğŸ“„ {category} - {file_path.name}")

        try:
            with open(file_path, 'rb') as f:
                data = f.read()
                size = len(data)
                header = data[:32]  # å‰32å­—èŠ‚ä½œä¸ºç‰¹å¾
                entropy = calculate_entropy(data)

                file_info[file_path.name] = {
                    'category': category,
                    'size': size,
                    'header': header,
                    'entropy': entropy
                }

                print(f"  å¤§å°: {size:,} å­—èŠ‚")
                print(f"  ç†µå€¼: {entropy:.3f}")

        except Exception as e:
            print(f"  âŒ è¯»å–å¤±è´¥: {e}")

    # åˆ†ææ–‡ä»¶é—´çš„ç›¸ä¼¼æ€§
    print(f"\nğŸ“Š æ–‡ä»¶ç›¸ä¼¼æ€§åˆ†æ:")
    print("-" * 40)

    # æŒ‰å¤§å°åˆ†ç»„
    size_groups = {}
    for name, info in file_info.items():
        size = info['size']
        if size not in size_groups:
            size_groups[size] = []
        size_groups[size].append(name)

    print("æŒ‰å¤§å°åˆ†ç»„:")
    for size, files in size_groups.items():
        print(f"  {size:,} å­—èŠ‚: {', '.join(files)}")

    # æ£€æŸ¥æ–‡ä»¶å¤´ç›¸ä¼¼æ€§
    print("\næ–‡ä»¶å¤´åˆ†æ:")
    headers = {}
    for name, info in file_info.items():
        header_hex = binascii.hexlify(info['header'][:16]).decode()
        if header_hex not in headers:
            headers[header_hex] = []
        headers[header_hex].append(name)

    for header, files in headers.items():
        print(f"  å¤´éƒ¨ {header[:24]}...: {', '.join(files)}")

def create_analysis_report():
    """åˆ›å»ºåˆ†ææŠ¥å‘Š"""
    output_dir = Path("../output")
    output_dir.mkdir(exist_ok=True)

    report_file = output_dir / "hd_analysis_report.md"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# HDæ–‡ä»¶æ ¼å¼åˆ†ææŠ¥å‘Š\n\n")
        f.write("## åˆ†ææ¦‚è¿°\n\n")
        f.write("æœ¬æŠ¥å‘Šåˆ†æäº†å‚å•†å®šåˆ¶çš„.hdæ–‡ä»¶æ ¼å¼ï¼Œç”¨äºç†è§£å¤©é—®Blockè½¯ä»¶çš„å›ºä»¶æ ¼å¼ã€‚\n\n")
        f.write("## å‘ç°\n\n")
        f.write("### æ–‡ä»¶æ ¼å¼ç‰¹å¾\n")
        f.write("- æ–‡ä»¶æ‰©å±•å: .hd\n")
        f.write("- ç”¨é€”: æœºå™¨äººæ¡Œå® ç‹—å›ºä»¶\n")
        f.write("- å¼€å‘å·¥å…·: å¤©é—®Blockå›¾å½¢åŒ–ç¼–ç¨‹è½¯ä»¶\n\n")
        f.write("### ç ´è§£å»ºè®®\n\n")
        f.write("1. **ç¡¬ä»¶åˆ†æ**: åˆ†æä¸“ç”¨ä¸‹è½½å™¨çš„ç”µè·¯å’Œé€šä¿¡åè®®\n")
        f.write("2. **æ–‡ä»¶æ ¼å¼**: ç»§ç»­æ·±å…¥åˆ†æ.hdæ–‡ä»¶çš„å†…éƒ¨ç»“æ„\n")
        f.write("3. **åè®®é€†å‘**: ç›‘å¬ä¸‹è½½è¿‡ç¨‹ä¸­çš„é€šä¿¡æ•°æ®\n")
        f.write("4. **æ›¿ä»£æ–¹æ¡ˆ**: å¼€å‘è‡ªå·±çš„çƒ§å½•å·¥å…·\n\n")
        f.write("### ä¸‹ä¸€æ­¥è¡ŒåŠ¨\n\n")
        f.write("- [ ] è´­ä¹°æˆ–å€Ÿç”¨ä¸“ç”¨ä¸‹è½½å™¨è¿›è¡Œç¡¬ä»¶åˆ†æ\n")
        f.write("- [ ] ä½¿ç”¨é€»è¾‘åˆ†æä»ªç›‘å¬é€šä¿¡åè®®\n")
        f.write("- [ ] å¼€å‘.hdæ–‡ä»¶è½¬æ¢å·¥å…·\n")
        f.write("- [ ] åˆ¶ä½œå…¼å®¹çš„çƒ§å½•å·¥å…·\n")

    print(f"\nğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å‚å•†HDæ–‡ä»¶æ·±åº¦åˆ†æå·¥å…·")
    print("=" * 60)
    print("ç›®æ ‡: ç ´è§£å¤©é—®Blockçš„.hdæ–‡ä»¶æ ¼å¼")
    print("=" * 60)

    # åˆ†æå•ä¸ªæ–‡ä»¶
    base_path = Path(r"d:\repositories\robot_dog\3.æ¡Œå® ç‹—ä»£ç åŠæŒ‡ä»¤é›†ï¼ˆ3ä»£~9ä»£çš„æŒ‡ä»¤é›†åŠè¯­éŸ³æ¨¡å—ä»£ç ç­‰ï¼‰\2.è…¾å“¥æ¯ä»£è¯­éŸ³æ¨¡å—å¼€æºä»£ç ")

    # å…ˆåˆ†æä¸€ä¸ªå…·ä½“æ–‡ä»¶
    target_file = base_path / "éAI" / "è…¾å“¥9ä»£æ¡Œå® ç‹—.hd"
    if target_file.exists():
        success = analyze_hd_file_basic(str(target_file))
        if success:
            # æ¯”è¾ƒå¤šä¸ªæ–‡ä»¶
            compare_multiple_files()

            # åˆ›å»ºåˆ†ææŠ¥å‘Š
            create_analysis_report()
        else:
            print("âŒ å•æ–‡ä»¶åˆ†æå¤±è´¥ï¼Œè·³è¿‡åç»­åˆ†æ")
    else:
        print(f"âŒ ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {target_file}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
    print("ğŸ’¡ åŸºäºåˆ†æç»“æœï¼Œæ¨èçš„ç ´è§£æ­¥éª¤:")
    print("   1. è·å–ä¸“ç”¨ä¸‹è½½å™¨è¿›è¡Œç¡¬ä»¶åˆ†æ")
    print("   2. ä½¿ç”¨ç¤ºæ³¢å™¨åˆ†æé€šä¿¡ä¿¡å·")
    print("   3. å¼€å‘æ›¿ä»£çš„çƒ§å½•å·¥å…·")
    print("   4. å®ç°è‡ªå®šä¹‰å›ºä»¶å¼€å‘ç¯å¢ƒ")

if __name__ == "__main__":
    main()
